from builtins import map
from builtins import str
import logging
import tarfile
import abc
import json
import subprocess
import os
import copy
import random
from stallion.auth import Authorizer
from stallion import util
from .base import ServiceVersion, BaseVersionedArtifactDeployer


class DataflowServiceVersion(ServiceVersion):
    COMPONENT_NAME_LABEL = 'stallion-component-name'
    CODE_VERSION_LABEL = 'stallion-code-version'

    """ A ServiceVersion instance specific to streaming dataflow jobs.
        :param job: the job
        :type job: dict (for structure, see: https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs#Job)
    """
    def __init__(self, job):
        self.job = job

        super(DataflowServiceVersion, self).__init__(
            service_name=self.component_name,
            version=self.code_version)

    # This deployer does not deal with inactive jobs.  The is_active property
    # is required, so set it to true
    is_active = True

    @property
    def labels(self):
        return self.job.get('labels', {})

    @property
    def component_name(self):
        return self.labels.get(self.COMPONENT_NAME_LABEL)

    @property
    def code_version(self):
        raw_version = self.labels.get(self.CODE_VERSION_LABEL)
        return util.decode_version(raw_version) if raw_version else None

    @property
    def job_name(self):
        return self.job.get('name')

    @property
    def job_location(self):
        return self.job.get('location')

    @classmethod
    def build_labels(cls, component_name, code_version):
        return {
            cls.COMPONENT_NAME_LABEL: component_name,
            cls.CODE_VERSION_LABEL: code_version,
        }

    @property
    def job_type(self):
        return self.job.get('type')

    @property
    def is_streaming(self):
        return self.job_type == 'JOB_TYPE_STREAMING'

    @property
    def job_id(self):
        return self.job.get('id')

    @property
    def current_state(self):
        return self.job.get('currentState')

    def with_draining_state(self):
        job_copy = copy.deepcopy(self.job)

        job_copy['requestedState'] = 'JOB_STATE_DRAINING'

        return DataflowServiceVersion(job_copy)

    def __repr__(self):
        return '{}/{}:{}'.format(
            self.job_id,
            self.component_name,
            self.version)


class GoogleCloudDataflowStreamingPipelineDeployer(BaseVersionedArtifactDeployer):
    def __init__(self, *args, **kwargs):
        super(GoogleCloudDataflowStreamingPipelineDeployer, self).__init__(*args, **kwargs)

        self.session = Authorizer.read_write().get_access_session()

    @abc.abstractmethod
    def _launch_job(self, component, workdir, dry_run):
        pass

    def _get_page(self, nextPageToken):
        logging.info('Retrieving page for token %s', nextPageToken)

        params = {
            'filter': 'ACTIVE'
        }
        if nextPageToken:
            params['pageToken'] = nextPageToken

        # documentation for this endpoint:
        # https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.jobs/aggregated
        response = self.session.get(
            'https://dataflow.googleapis.com/v1b3/projects/{project_id}/jobs:aggregated'.format(
                project_id=self.project.id),
            params=params
        )
        response.raise_for_status()

        return response.json()

    def _get_job_details(self, job_id, location):
        params = {
            'view': 'JOB_VIEW_SUMMARY',
        }

        # documentation for this endpoint:
        # https://cloud.google.com/dataflow/docs/reference/rest/v1b3/projects.locations.jobs/get
        response = self.session.get(
            'https://dataflow.googleapis.com/v1b3/projects/{project_id}/locations/{location}/jobs/{job_id}'.format(
                project_id=self.project.id,
                location=location,
                job_id=job_id),
            params=params
        )
        response.raise_for_status()

        return response.json()

    def get_deployed_versions(self, relevant_components):
        """ Return value should be a list of ServiceVersion objects
            for services/versions that the deployer has records for.
        """

        jobs = []
        page = None

        while (page is None) or page.get('nextPageToken'):
            page = self._get_page((page or {}).get('nextPageToken'))

            # the list endpoint provides brief summaries of the jobs, and these do not include
            # the labels and other information that we need for assessing the job.  We pull
            # the summaries first, and then retrieve detailed information later on a job-by-job
            # basis for those jobs that might be applicable.
            job_summaries = [ DataflowServiceVersion(job) for job in page.get('jobs', []) ]

            jobs += [
                DataflowServiceVersion(self._get_job_details(job.job_id, job.job_location))
                for job in job_summaries
                if job.is_streaming
            ]

        result = [ job for job in jobs if job.code_version ]
        logging.info("Found jobs: %s", result)

        return result

    def delete_stale_versions(
            self,
            components,
            max_age_seconds,
            clean_unknown_components,
            dry_run):
        logging.info('No stale-version logic for deployer %s', self.type)

    def _do_deploy(self, component, existing_versions, workdir, dry_run):
        if len(existing_versions) > 1:
            logging.warning(
                '''Multiple existing versions found for component `%s`!
                   This should theoretically have been impossible, but I
                   checked anyway.  I'm going to drain all of them.
                ''',
                component)

        for version in existing_versions:
            if dry_run:
                logging.info(
                    '--dry_run: not draining existing job %s',
                    version)
                continue

            if version.current_state != 'JOB_STATE_RUNNING':
                logging.info(
                    'Job %s is in state %s: not eligible for draining.',
                    version.job_id,
                    version.current_state
                )
                continue

            logging.info('Draining job %s', version.job_id)
            self._drain_job(version)

        self._launch_job(component, workdir, dry_run)

    def _drain_job(self, version):
        response = self.session.put(
            'https://dataflow.googleapis.com/v1b3/projects/{project_id}/jobs/{job_id}'.format(
                project_id=self.project.id,
                job_id=version.job_id),
            params={'location': version.job_location},
            json=version.with_draining_state().job,
        )
        response.raise_for_status()

        return response.json()

    @staticmethod
    def _random_hash(num_chars):
        # Construct a random hash of the requested length.  We get this by
        # generating a random int between 0 and (2^(4*length) - 1), and then
        # formatting as hexadecimal.
        fmt = '%0{}x'.format(num_chars)
        return (fmt % random.randint(0, 1<<(num_chars*4) - 1))

    def _build_job_name(self, component):
        return component.name + '-' + util.encode_version(component.code_version) + '-' + self._random_hash(4)

    def _build_labels(self, component):
        result = DataflowServiceVersion.build_labels(
            component_name=component.name,
            code_version=util.encode_version(component.code_version))

        user_labels = component.descriptor['dataflow'].get('labels', {})
        invalid_labels = [ key for key in user_labels if key in result ]
        if invalid_labels:
            raise Exception(
                'Invalid use of reserved labels `{}`'.format(
                invalid_labels))

        result.update(user_labels)

        return result


class DataflowJavaStreamingPipelineDeployer(GoogleCloudDataflowStreamingPipelineDeployer):
    type = 'dataflow-streaming-java'

    def _launch_job(self, component, workdir, dry_run):
        # TODO: should verify that jar exists when running in dry_run mode?
        classpath = None
        if not dry_run:
            package_dir = self._prepare_deployment(component, workdir)

            classpath = os.path.join(
                package_dir,
                component.settings.get('jar_dir_within_artifact', 'lib'),
                '*')

        launch_cmd = [
            'java',
            '-cp', classpath,
            component.settings['main_class'],
            '--runner=dataflow',
            '--streaming=true',
            '--project={}'.format(self.project.id),
            '--jobName={}'.format(self._build_job_name(component)),
            '--labels={}'.format(json.dumps(self._build_labels(component))),
            '--region={}'.format(component.descriptor['dataflow']['region']),
        ]

        launch_cmd += self._optional_args(
            component,
            ('service_account', '--serviceAccount'),
            ('zone', '--zone'),
            ('autoscaling_algorithm', '--autoscalingAlgorithm'),
            ('num_workers', '--numWorkers'),
            ('max_num_workers', '--maxNumWorkers'),
            ('use_public_ips', '--usePublicIps', lambda x: str(x).lower()),
            ('subnetwork', '--subnetwork'),
        )

        launch_cmd += component.descriptor.get('dataflow', {}).get('job_arguments', [])

        logging.info('Running: %s', ' '.join(map(str, launch_cmd)))

        if(dry_run):
            logging.info('--dry_run: Not launching job.')
            return

        subprocess.check_call(
            launch_cmd,
            cwd=workdir)

    @classmethod
    def _optional_arg(cls, component, descriptor_name, cmdline_name, formatter=None):
        value = component.descriptor.get('dataflow', {}).get(descriptor_name)
        if value is None:
            return []

        return [ '{}={}'.format(
            cmdline_name,
            value if not formatter else formatter(value)) ]

    @classmethod
    def _optional_args(cls, component, *args):
        return [
            item for arg in args
            for item in cls._optional_arg(
                component,
                *arg)
        ]

    def _prepare_deployment(self, component, workdir):
        artifact = self._download_artifact(component, workdir)

        extract_to = os.path.join(workdir, '_extracted')

        with tarfile.open(artifact) as tf:
            tf.extractall(path=os.path.join(extract_to))

        result = os.path.join(extract_to, os.path.basename(artifact))
        if result.endswith('.tar.gz'):
            return result[:-len('.tar.gz')]
        elif result.endswith('.tgz'):
            return result[:-len('.tgz')]
        elif result.endswith('.tar'):
            return result[:-len('.tgz')]
        else:
            raise Exception(
                'Unrecognized extension for artifact {}'.format(os.path.basename(artifact))
            )

    def _download_artifact(self, component, workdir):
        path_template = component.settings['artifact_path_template']
        code_version_directive = '{code_version}'
        if code_version_directive not in path_template:
            raise Exception(
                'Invalid jar path template: {} (missing {})'.format(
                    code_version_directive,
                    path_template))

        path = path_template.replace(
            code_version_directive,
            component.code_version
        )

        return util.gcs_download(
            bucket_name=component.settings['artifact_bucket'],
            path=path,
            local_dir=workdir)
